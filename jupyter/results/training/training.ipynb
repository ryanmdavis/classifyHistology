{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Performance on Test Dataset\n",
    "\n",
    "![training data](training.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method and Results\n",
    "[Net1](../../methods/architecture/architecture.ipynb) was trained with a variable learning rate of 0.001 until iteration 5, 0.0001 until iteration 50, and 0.00005 for subsequent iterations. Dropout regularization was used for all fully connected and convolution layers except for the output fully connected layer. The probability of keeping a neuron was set at 1, 0.6, or 0.4. The resulting test dataset classification accuracy (Figure a) and loss (Figure b) are shown as a function of training iteration and neuron keep probability, P, in the figure.\n",
    "\n",
    "As can be seen in Figure a, at the final iteration a P of 1 and 0.6 resuled in a test dataset classification accuracy of 0.985, while further regularization (P = 0.4) resulted in poorer accuracy of 0.93. In Figure b, the lowest loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
